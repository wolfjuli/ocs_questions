\documentclass{report}


\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\newcommand{\partder}[1]{\frac{\partial h^l}{\partial #1}}

\graphicspath{{figures/}}

\title{OCS Hints for Questions}
\author{Julian Wolf}
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Derivations}
Die Antworten sind teilweise unvollständig, einerseits, weil er die Antworten als "Eh Klar" abgestempelt hat, anderer seits weil er so schnell durchging, dass ein Mitschreiben nicht mehr möglich war. 

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Draw level lines and arrows
\begin{itemize}
\item objective function is the function we want to minimize
\item constraint set is a set of functions
\item optimal solution: find $f(x^*) \leq f(x), \forall x \in X$
\item level set: compareable to level lines of terrain, convex function $=>$ convex level set (but there are non convex fct with convex level sets), 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item 
\begin{itemize}
\item \textbf{Linear}: Objective Function and Constraints may only be linear
$min\ c^Tx, s.t.\ Ax \leq b, x \geq 0$\\
Polynomial solvable
\item \textbf{Non Linear}: Objective Function and Constriants may  be non linear
$min\ \frac{1}{2} x^TQx + c^Tx, s.t.\ Ax \leq b, Ex = d$\\
Q symmetrical and pos. definite, polynomial solvable
\item \textbf{Quadratic}: objective function is quadratic, constraints are linear	
$min_{x \in \mathbb{R}}\ f_0(x) \texttt{ (objective)},$\\
$s.t.\ f_i(x) \leq i=0..m \texttt{ (contraints)}$\\
polynomial time
\item \textbf{convex set}: 
$\alpha x + (1 - \alpha)y \ in X, \forall x, y \in X, \alpha \in [0, 1]$
\item \textbf{convex fct}:
$f(\alpha x + (1 - \alpha)y) \leq \alpha f(x) + (1 - \alpha)f(y) , \forall x, y \in X, \alpha \in [0, 1]$ 
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item 
\begin{itemize}
	\item When hessian is strictly positive, it is a strict global maximum
	\item \textbf{unconst Local minimum:} $f(x^*) \leq f(x), \forall x \texttt{ with } || x - x^* || \leq \varepsilon$
	\item \textbf{unconst global minimum:} $f(x^*) \leq f(x), \forall x \in \mathbb{R}$
\end{itemize}

\begin{figure}[H]
\includegraphics[width=\textwidth]{loc_glob_min.png}
\caption{Local/Global minimas\label{fig:min}}
\end{figure}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item 
\begin{itemize}
\item If positive and negative Eigenvalues, we can not define convexity
\item First order necessessary optimality condition: $\nabla f(x^*) = 0$
\item Second order necessessary optimality condition: $\nabla^2 f(x^*) $ is positive semi definite

\begin{figure}[H]
\includegraphics[width=\textwidth]{cond_quad_q.png}
\caption{Different scenarios for Q\label{fig:cond_quad}}
\end{figure}	
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item 
\begin{itemize}
\item Descent direction: angle of step and derivation direction $< 90^\circ$
\item General form of gradient method: \\
1. Choose an initial vector $x ^0 \in \mathbb{R}^n$\\
2. Choose a descent direction $d^k$ that satisfies $\nabla f (x^k)'  d^k < 0$\\
3. Choose a positive step size $\alpha^k$\\
4. Compute the new vector as\\
$x^{k+1} = x^k + \alpha^k d^k$\\
5. Set $k = k + 1$ and goto 2
\begin{figure}[H]
\includegraphics[width=\textwidth]{desc_dir.png}
\caption{Simple descent direction\label{fig:desc_dir}}
\end{figure}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item $d^k = -D^k \nabla f (x^k )$
\begin{itemize}
	\item Identity: $D^k = I$, = Gradient descent, zig zagging problem, very bad on Rosenbrock Fct
	\item Hessian: $D^k = \nabla^2 f(x^k)$, = Newtons method, very fast convergence, very good on rosenbrock, unstable in despite of initial values (may diverge or find local maxima instead of minima), con: calculation of inverse of hessian - very expensive in large networks
	\item Diagonal Hessian (approximation of Newton): $d_i^k \approx \left(
\frac{\partial^2  f (x^k)}{(\partial x_i)^2}
\right)^{-1}$, very bad performance on Rosenbrock, 
	\item Gauss Newton method: Too complicated to remember, replace $D^k$ with non linear least square problem, even better performance on rosenbrock then newton, con: again calculation of inverse, but not of hessian
	\item \textbf{Step size} $\alpha$: 
	\begin{itemize}
		\item \textbf{Minimization rule:} choose $\alpha$ such that $f(x + \alpha d)$ is minimized along $d$. Hard if $f$ is complicated
		\item \textbf{Limited minimization rule:} iterative: start small and increase size of $\alpha$ until $f(x)$ is bigger then before, then choose the previous. Easy to implement
		\item \textbf{Armijo rule:} it is not sufficient that $f(x^{k+1}) < f(x^k)$, thus, the step sizes $\beta^ms$ for $m = 0,1,...$ are chosen such that the energy decrease is sufficiently large (dependent on derivation of $f(x)$, formula too complicated), or graphical:
	\begin{figure}[H]
	\includegraphics[width=\textwidth]{armijo.png}
	\caption{Graphical representation of the idea of Armijo\label{fig:desc_dir}}
	\end{figure}
		
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item 
\begin{itemize}
	\item \textbf{Linear:} $\limsup_{k \rightarrow \infty} \frac{e(x^{k+1})}{e(x^k)}\leq \beta$ (blue line)
	\item \textbf{superlinear:} $\limsup_{k \rightarrow \infty} \frac{e(x^{k+1})}{e(x^k)^p} < \infty$ (red line)
	\item \textbf{sublinear:} $\limsup_{k \rightarrow \infty} \frac{e(x^{k+1})}{e(x^k)} = 1$ (black line)
\end{itemize}

\begin{figure}[H]
\includegraphics[width=10cm]{convergence.png}
\caption{Graphical representation of linear, superinear and sublinear convergence \label{fig:desc_dir}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Energy convergence 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \sout{too fast}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \sout{polynomial euqations, distance to std. Newton}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item incremental of gauss newton
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \sout{too fast}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \sout{iterative}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item nesterov in gradient, heavy-ball just in point
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item in subspace reduce to eq, what is a subspace?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item First pages of slide 10
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item middle/end of pages slide 10 - start in interior and just take small steps $->$ we can ignore constraint under these conditions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \sout{too fast}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item see figure~\ref{fig:ex1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item see figure~\ref{fig:ex2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item see figure~\ref{fig:ex2}


\end{enumerate}
\begin{figure}[H]
\includegraphics[width=\textwidth]{2017_01_24-ex1.jpg}
\caption{Example1,  24.01.2017\label{fig:ex1}}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{2017_01_24-ex2.jpg}
\caption{Example 2,  24.01.2017\label{fig:ex2}}
\end{figure}
\end{document}
